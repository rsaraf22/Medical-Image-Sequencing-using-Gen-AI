{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "647fbc6a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c8d4aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51c8d4aa",
    "outputId": "26da6294-ee9d-47e1-bb15-84c4b86dae80"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers\n",
    "!pip install torch torchvision timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab052b",
   "metadata": {
    "id": "c9ab052b"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import timm\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "from skimage.metrics import structural_similarity as ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ab674c",
   "metadata": {},
   "source": [
    "## Data extraction using Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a48ecd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 934,
     "referenced_widgets": [
      "6f41e799c4ba44e098fc4b16ff49bf55",
      "d8da25ac8ac64c23875dc80eac6dc331",
      "886073ad45714e399cb0d2948bb73fb9",
      "a137223106c84c07bd67c1e07463ee25",
      "156ca64c104c4bf8ac38de58d447e445",
      "e4a114db3221401db7dcc8a6450ae383",
      "b9b2f9be4bb240a1bddcfd15e5c8b006",
      "b1451f89734c41b883962f6a628e5aab",
      "137ec9d9715c40a69c347a84f7ef0cc1",
      "01d1ede7d1e644aeabcaa66b0961772a",
      "cfde66647bb941a08378e6d0ea5adc85",
      "9c264bf0133147f4987773fcde26a6cf",
      "4303dfdaaaf548aab64a12895bc0b52b",
      "a69987aa734149af9190c89a763f0fa7",
      "c22a919afb924e508fd6a103f4bccb08",
      "c93918ee586847d9be2b308b6c5e83d6",
      "e9bc7a91125a4702b64ca385c4dd9cbf",
      "b26f9152c2464df59fa085cf309b5133",
      "a853575b47ec459cbcb834288d5dfd58",
      "f80ad230e54a4470997d9bb42e2c7f35",
      "a7ea13b5e7e14f4487fac07678c9b741",
      "05d808ba79034d10a4b13bfee51c957a"
     ]
    },
    "id": "c2a48ecd",
    "outputId": "a49ac636-5fab-46fb-8022-b1833d1bdf0d"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, max_images_per_patient=20):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.max_images_per_patient = max_images_per_patient\n",
    "        self.patients = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patients)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_dir = os.path.join(self.root_dir, self.patients[idx])\n",
    "        image_paths = sorted([os.path.join(patient_dir, f) for f in os.listdir(patient_dir) if os.path.isfile(os.path.join(patient_dir, f))])[:self.max_images_per_patient]\n",
    "\n",
    "        images = []\n",
    "        for img_path in image_paths:\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            if img.shape[-1] == 1:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            images.append(img)\n",
    "\n",
    "        images_tensor = torch.stack(images)\n",
    "        return images_tensor, self.patients[idx]\n",
    "\n",
    "transform = Compose([ToTensor()])\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = CustomDataset(\"/scratch/ajalagam/Formatted_Data_PNG\", transform=transform) #give your path to the dataset\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "vit_model = ViTForImageClassification.from_pretrained(model_name)\n",
    "vit_model.classifier = nn.Identity()  #to get feature vector\n",
    "vit_model.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f88256",
   "metadata": {},
   "outputs": [],
   "source": [
    "Validdataset = CustomDataset(\"/scratch/ajalagam/Valid_Dataset\", transform=transform)\n",
    "\n",
    "# give your path to your  valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8cfecb",
   "metadata": {},
   "source": [
    "## Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b089e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_features = {}\n",
    "for images, patient_id in DataLoader(dataset, batch_size=1):\n",
    "    images = images.squeeze(0)  \n",
    "    with torch.no_grad():\n",
    "      features = vit_model(images).logits\n",
    "    patient_features[patient_id] = features\n",
    "\n",
    "# patient_features dictionary contains features for each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MwWsojCHA7oP",
   "metadata": {
    "id": "MwWsojCHA7oP"
   },
   "outputs": [],
   "source": [
    "valid_patient_features = {}\n",
    "for val_images, val_patient_id in DataLoader(Validdataset, batch_size=1):\n",
    "    val_images = val_images.squeeze(0) \n",
    "    with torch.no_grad():\n",
    "      val_features = vit_model(val_images).logits\n",
    "    valid_patient_features[val_patient_id] = val_features\n",
    "\n",
    "# valid_patient_features dictionary contains features for each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BFIb1bfqvyAg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BFIb1bfqvyAg",
    "outputId": "ad676a92-0093-4853-bafa-0eb465cc1d04"
   },
   "outputs": [],
   "source": [
    "len(patient_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_patient_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f97e11",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4MwcZSfQsYOk",
   "metadata": {
    "id": "4MwcZSfQsYOk"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        hidden_dim = 512\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(768, feature_dim * 2),\n",
    "            \n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 3 * 224 * 224),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aggregate features across the batch\n",
    "        x = x.mean(dim=0, keepdim=True)  # Taking the mean of the batch\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = h.chunk(2, dim=-1)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decoder(z).view(3, 224, 224), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac7cf1f",
   "metadata": {
    "id": "2ac7cf1f"
   },
   "outputs": [],
   "source": [
    "vae = VAE(feature_dim=512).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f051d",
   "metadata": {
    "id": "5f8f051d"
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def vae_loss(reconstructed, target, mu, log_var):\n",
    "    reconstruction_loss = nn.functional.mse_loss(reconstructed, target)\n",
    "    kl_div = -0.5* 0.001 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return reconstruction_loss + kl_div\n",
    "\n",
    "# optimizer\n",
    "vae_optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f7b8b",
   "metadata": {},
   "source": [
    "\n",
    "## Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec6331",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "id": "3dec6331",
    "outputId": "96e96e28-35a3-4e2c-a9a9-e8714b426b86"
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "losses = []\n",
    "\n",
    "num_epochs = 100\n",
    "vae.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for patient_id, features in patient_features.items():\n",
    "        # Split features into input (first 19 images) and target (20th image)\n",
    "        input_features = features[:-1]\n",
    "        last_image_index = features.shape[0]\n",
    "        folder = int(patient_id[0]) # First 19 images\n",
    "        input_features = input_features.to(device)\n",
    "        images_tensor, patient_id2 = dataset[count]\n",
    "        if last_image_index > 20:\n",
    "            last_image_index = 20\n",
    "        specific_image_tensor = images_tensor[-1]\n",
    "        specific_image_tensor = specific_image_tensor.to(device)\n",
    "        reconstructed, mu, log_var = vae(input_features)\n",
    "       \n",
    "    # Compute VAE loss\n",
    "        loss = vae_loss(reconstructed, specific_image_tensor, mu, log_var)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        vae_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        vae_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "        t_loss = total_loss / count if count else 0\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {t_loss:.4f}\")\n",
    "    losses.append(t_loss)\n",
    "\n",
    "# Plot the loss values\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "torch.save(vae.state_dict(), '/scratch/ajalagam/Savedvae_model.pth') # give path where you want save the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f10224",
   "metadata": {},
   "source": [
    "## Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a19463",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "val_losses = []\n",
    "\n",
    "val_num_epochs = 100\n",
    "vae.to(device)\n",
    "for epoch in range(val_num_epochs):\n",
    "    val_total_loss = 0.0\n",
    "    count = 0\n",
    "    for val_patient_id, val_features in valid_patient_features.items():\n",
    "        val_input_features = val_features[:-1]\n",
    "        val_last_image_index = val_features.shape[0]\n",
    "        val_folder = int(val_patient_id[0]) # First 19 images\n",
    "      \n",
    "        val_input_features = val_input_features.to(device)\n",
    "        val_images_tensor, val_patient_id2 = Validdataset[count]\n",
    "        if val_last_image_index > 20:\n",
    "            val_last_image_index = 20\n",
    "       \n",
    "        val_specific_image_tensor = val_images_tensor[-1]\n",
    "        val_specific_image_tensor = val_specific_image_tensor.to(device)\n",
    "        val_reconstructed, val_mu, val_log_var = vae(val_input_features)\n",
    "        val_loss = vae_loss(val_reconstructed, val_specific_image_tensor, val_mu, val_log_var)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        val_total_loss += val_loss.item()\n",
    "        count += 1\n",
    "        v_loss = val_total_loss / count if count else 0\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {v_loss:.4f}\")\n",
    "    val_losses.append(v_loss)\n",
    "\n",
    "# Plot the loss values\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Validation finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6141fa67",
   "metadata": {},
   "source": [
    "## Output Validation and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c4c1b",
   "metadata": {
    "id": "9d9c4c1b"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae.to(device)\n",
    "\n",
    "key = ('80',)\n",
    "\n",
    "features_tensor = patient_features[key].to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ba653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image1 = cv2.imread('/scratch/ajalagam/Formatted_Data_PNG/80/18.png')  \n",
    "image2 = cv2.imread('/scratch/ajalagam/Formatted_Data_PNG/80/19.png') \n",
    "image3 = cv2.imread('/scratch/ajalagam/Formatted_Data_PNG/80/20.png')\n",
    "\n",
    "image1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "image2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "image3_rgb = cv2.cvtColor(image3, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4)) \n",
    "\n",
    "axes[0].imshow(image1_rgb)\n",
    "axes[0].set_title(\"Image 1\")\n",
    "axes[0].axis('off') \n",
    "\n",
    "axes[1].imshow(image2_rgb)\n",
    "axes[1].set_title(\"Image 2\")\n",
    "axes[1].axis('off')  \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31945ac2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "31945ac2",
    "outputId": "475d0d53-9902-4d09-d948-6c17d5c87092",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed_images, _, _ = vae(features_tensor)\n",
    "reconstructed_images = reconstructed_images.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "reconstructed_images = (reconstructed_images * 255).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0ad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4)) \n",
    "axes[0].imshow(image3_rgb)\n",
    "axes[0].set_title(\"Next Image in Sequence\")\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(reconstructed_images)\n",
    "axes[1].set_title(\"Generated Next Image in Sequence\")\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wYm7jjikqS9k",
   "metadata": {
    "id": "wYm7jjikqS9k"
   },
   "outputs": [],
   "source": [
    "\n",
    "gray1 = cv2.cvtColor(image3, cv2.COLOR_BGR2GRAY)\n",
    "gray1 = cv2.resize(gray1, (224, 224))\n",
    "gray2 = cv2.cvtColor(reconstructed_images, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "similarity, diff = ssim(gray1, gray2, full=True)\n",
    "\n",
    "plt.imshow(diff, cmap='gray')\n",
    "plt.title(\"Difference Image\")\n",
    "plt.show()\n",
    "\n",
    "print(\"SSIM:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae.to(device)\n",
    "\n",
    "key = ('22',)\n",
    "\n",
    "features_tensor = patient_features[key].to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the images\n",
    "image4 = cv2.imread('/scratch/ajalagam/Formatted_Data_PNG/22/18.png')  \n",
    "image5 = cv2.imread('/scratch/ajalagam/Formatted_Data_PNG/22/19.png') \n",
    "image6 = cv2.imread('/scratch/ajalagam/Formatted_Data_PNG/22/20.png')  \n",
    "\n",
    "image4_rgb = cv2.cvtColor(image4, cv2.COLOR_BGR2RGB)\n",
    "image5_rgb = cv2.cvtColor(image5, cv2.COLOR_BGR2RGB)\n",
    "image6_rgb = cv2.cvtColor(image6, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(image4_rgb)\n",
    "axes[0].set_title(\"Image 1\")\n",
    "axes[0].axis('off') \n",
    "\n",
    "axes[1].imshow(image5_rgb)\n",
    "axes[1].set_title(\"Image 2\")\n",
    "axes[1].axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d1b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed_images, _, _ = vae(features_tensor)\n",
    "reconstructed_images = reconstructed_images.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "reconstructed_images = (reconstructed_images * 255).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe1dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(image6_rgb)\n",
    "axes[0].set_title(\"Next Image in Sequence\")\n",
    "axes[0].axis('off')  \n",
    "\n",
    "axes[1].imshow(reconstructed_images)\n",
    "axes[1].set_title(\"Generated Next Image in Sequence\")\n",
    "axes[1].axis('off')  \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gray1 = cv2.cvtColor(image6, cv2.COLOR_BGR2GRAY)\n",
    "gray1 = cv2.resize(gray1, (224, 224))\n",
    "gray2 = cv2.cvtColor(reconstructed_images, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "similarity, diff = ssim(gray1, gray2, full=True)\n",
    "\n",
    "plt.imshow(diff, cmap='gray')\n",
    "plt.title(\"Difference Image\")\n",
    "plt.show()\n",
    "\n",
    "print(\"SSIM:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8808cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae.to(device)\n",
    "\n",
    "key = ('30',)\n",
    "\n",
    "features_tensor = patient_features[key].to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ed407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image7 = cv2.imread('/scratch/ajalagam/Formatted_Data_PNG/30/18.png')\n",
    "image8 = cv2.imread('/scratch/ajalagam/Formatted_Data_PNG/30/19.png') \n",
    "image9 = cv2.imread('/scratch/ajalagam/Formatted_Data_PNG/30/20.png') \n",
    "image7_rgb = cv2.cvtColor(image7, cv2.COLOR_BGR2RGB)\n",
    "image8_rgb = cv2.cvtColor(image8, cv2.COLOR_BGR2RGB)\n",
    "image9_rgb = cv2.cvtColor(image9, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))  \n",
    "axes[0].imshow(image7_rgb)\n",
    "axes[0].set_title(\"Image 1\")\n",
    "axes[0].axis('off') \n",
    "\n",
    "axes[1].imshow(image8_rgb)\n",
    "axes[1].set_title(\"Image 2\")\n",
    "axes[1].axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be3f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed_images, _, _ = vae(features_tensor)\n",
    "reconstructed_images = reconstructed_images.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "reconstructed_images = (reconstructed_images * 255).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4)) \n",
    "\n",
    "axes[0].imshow(image9_rgb)\n",
    "axes[0].set_title(\"Next Image in Sequence\")\n",
    "axes[0].axis('off') \n",
    "\n",
    "axes[1].imshow(reconstructed_images)\n",
    "axes[1].set_title(\"Generated Next Image in Sequence\")\n",
    "axes[1].axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eac913",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gray1 = cv2.cvtColor(image9, cv2.COLOR_BGR2GRAY)\n",
    "gray1 = cv2.resize(gray1, (224, 224))\n",
    "gray2 = cv2.cvtColor(reconstructed_images, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "\n",
    "similarity, diff = ssim(gray1, gray2, full=True)\n",
    "\n",
    "plt.imshow(diff, cmap='gray')\n",
    "plt.title(\"Difference Image\")\n",
    "plt.show()\n",
    "\n",
    "print(\"SSIM:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913dc92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae.to(device)\n",
    "key = ('18',)\n",
    "features_tensor = patient_features[key].to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbbff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image10 = cv2.imread('/scratch/ajalagam/Formatted_Data_PNG/18/18.png') \n",
    "image11 = cv2.imread('/scratch/ajalagam/Formatted_Data_PNG/18/19.png')\n",
    "image12 = cv2.imread('/scratch/ajalagam/Formatted_Data_PNG/18/20.png')\n",
    "image10_rgb = cv2.cvtColor(image10, cv2.COLOR_BGR2RGB)\n",
    "image11_rgb = cv2.cvtColor(image11, cv2.COLOR_BGR2RGB)\n",
    "image12_rgb = cv2.cvtColor(image12, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4)) \n",
    "\n",
    "axes[0].imshow(image10_rgb)\n",
    "axes[0].set_title(\"Image 1\")\n",
    "axes[0].axis('off')  \n",
    "\n",
    "axes[1].imshow(image11_rgb)\n",
    "axes[1].set_title(\"Image 2\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d8dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed_images, _, _ = vae(features_tensor)\n",
    "reconstructed_images = reconstructed_images.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "reconstructed_images = (reconstructed_images * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51159caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(image12_rgb)\n",
    "axes[0].set_title(\"Next Image in Sequence\")\n",
    "axes[0].axis('off') \n",
    "\n",
    "axes[1].imshow(reconstructed_images)\n",
    "axes[1].set_title(\"Generated Next Image in Sequence\")\n",
    "axes[1].axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0500ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "gray1 = cv2.cvtColor(image12, cv2.COLOR_BGR2GRAY)\n",
    "gray1 = cv2.resize(gray1, (224, 224))\n",
    "gray2 = cv2.cvtColor(reconstructed_images, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "similarity, diff = ssim(gray1, gray2, full=True)\n",
    "\n",
    "plt.imshow(diff, cmap='gray')\n",
    "plt.title(\"Difference Image\")\n",
    "plt.show()\n",
    "\n",
    "print(\"SSIM:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d934e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae.to(device)\n",
    "\n",
    "key = ('37',)\n",
    "\n",
    "features_tensor = valid_patient_features[key].to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9443f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image13 = cv2.imread('/scratch/ajalagam/Valid_Dataset/37/14.png') \n",
    "image14 = cv2.imread('/scratch/ajalagam/Valid_Dataset/37/15.png')  \n",
    "image15 = cv2.imread('/scratch/ajalagam/Valid_Dataset/37/16.png') \n",
    "\n",
    "image13_rgb = cv2.cvtColor(image13, cv2.COLOR_BGR2RGB)\n",
    "image14_rgb = cv2.cvtColor(image14, cv2.COLOR_BGR2RGB)\n",
    "image15_rgb = cv2.cvtColor(image15, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(image13_rgb)\n",
    "axes[0].set_title(\"Image 1\")\n",
    "axes[0].axis('off')  \n",
    "\n",
    "axes[1].imshow(image14_rgb)\n",
    "axes[1].set_title(\"Image 2\")\n",
    "axes[1].axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d84326",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructed_images, _, _ = vae(features_tensor)\n",
    "reconstructed_images = reconstructed_images.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "reconstructed_images = (reconstructed_images * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027788d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))  \n",
    "\n",
    "axes[0].imshow(image15_rgb)\n",
    "axes[0].set_title(\"Next Image in Sequence\")\n",
    "axes[0].axis('off') \n",
    "\n",
    "axes[1].imshow(reconstructed_images)\n",
    "axes[1].set_title(\"Generated Next Image in Sequence\")\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gray1 = cv2.cvtColor(image15, cv2.COLOR_BGR2GRAY)\n",
    "gray1 = cv2.resize(gray1, (224, 224))\n",
    "gray2 = cv2.cvtColor(reconstructed_images, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "similarity, diff = ssim(gray1, gray2, full=True)\n",
    "\n",
    "plt.imshow(diff, cmap='gray')\n",
    "plt.title(\"Difference Image\")\n",
    "plt.show()\n",
    "\n",
    "print(\"SSIM:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e1eb3d",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1675417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb4b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(vae, input_size=(1, 768))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01d1ede7d1e644aeabcaa66b0961772a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05d808ba79034d10a4b13bfee51c957a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "137ec9d9715c40a69c347a84f7ef0cc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "156ca64c104c4bf8ac38de58d447e445": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4303dfdaaaf548aab64a12895bc0b52b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9bc7a91125a4702b64ca385c4dd9cbf",
      "placeholder": "​",
      "style": "IPY_MODEL_b26f9152c2464df59fa085cf309b5133",
      "value": "model.safetensors: 100%"
     }
    },
    "6f41e799c4ba44e098fc4b16ff49bf55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d8da25ac8ac64c23875dc80eac6dc331",
       "IPY_MODEL_886073ad45714e399cb0d2948bb73fb9",
       "IPY_MODEL_a137223106c84c07bd67c1e07463ee25"
      ],
      "layout": "IPY_MODEL_156ca64c104c4bf8ac38de58d447e445"
     }
    },
    "886073ad45714e399cb0d2948bb73fb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1451f89734c41b883962f6a628e5aab",
      "max": 502,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_137ec9d9715c40a69c347a84f7ef0cc1",
      "value": 502
     }
    },
    "9c264bf0133147f4987773fcde26a6cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4303dfdaaaf548aab64a12895bc0b52b",
       "IPY_MODEL_a69987aa734149af9190c89a763f0fa7",
       "IPY_MODEL_c22a919afb924e508fd6a103f4bccb08"
      ],
      "layout": "IPY_MODEL_c93918ee586847d9be2b308b6c5e83d6"
     }
    },
    "a137223106c84c07bd67c1e07463ee25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_01d1ede7d1e644aeabcaa66b0961772a",
      "placeholder": "​",
      "style": "IPY_MODEL_cfde66647bb941a08378e6d0ea5adc85",
      "value": " 502/502 [00:00&lt;00:00, 42.7kB/s]"
     }
    },
    "a69987aa734149af9190c89a763f0fa7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a853575b47ec459cbcb834288d5dfd58",
      "max": 345579424,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f80ad230e54a4470997d9bb42e2c7f35",
      "value": 345579424
     }
    },
    "a7ea13b5e7e14f4487fac07678c9b741": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a853575b47ec459cbcb834288d5dfd58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1451f89734c41b883962f6a628e5aab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b26f9152c2464df59fa085cf309b5133": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9b2f9be4bb240a1bddcfd15e5c8b006": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c22a919afb924e508fd6a103f4bccb08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7ea13b5e7e14f4487fac07678c9b741",
      "placeholder": "​",
      "style": "IPY_MODEL_05d808ba79034d10a4b13bfee51c957a",
      "value": " 346M/346M [00:01&lt;00:00, 268MB/s]"
     }
    },
    "c93918ee586847d9be2b308b6c5e83d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfde66647bb941a08378e6d0ea5adc85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8da25ac8ac64c23875dc80eac6dc331": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4a114db3221401db7dcc8a6450ae383",
      "placeholder": "​",
      "style": "IPY_MODEL_b9b2f9be4bb240a1bddcfd15e5c8b006",
      "value": "config.json: 100%"
     }
    },
    "e4a114db3221401db7dcc8a6450ae383": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9bc7a91125a4702b64ca385c4dd9cbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f80ad230e54a4470997d9bb42e2c7f35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
